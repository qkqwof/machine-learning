{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>feat_1</th>\n",
       "      <th>feat_2</th>\n",
       "      <th>feat_3</th>\n",
       "      <th>feat_4</th>\n",
       "      <th>feat_5</th>\n",
       "      <th>feat_6</th>\n",
       "      <th>feat_7</th>\n",
       "      <th>feat_8</th>\n",
       "      <th>feat_9</th>\n",
       "      <th>...</th>\n",
       "      <th>feat_85</th>\n",
       "      <th>feat_86</th>\n",
       "      <th>feat_87</th>\n",
       "      <th>feat_88</th>\n",
       "      <th>feat_89</th>\n",
       "      <th>feat_90</th>\n",
       "      <th>feat_91</th>\n",
       "      <th>feat_92</th>\n",
       "      <th>feat_93</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Class_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Class_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Class_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Class_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Class_1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 95 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  feat_1  feat_2  feat_3  feat_4  feat_5  feat_6  feat_7  feat_8  feat_9  \\\n",
       "0   1       1       0       0       0       0       0       0       0       0   \n",
       "1   2       0       0       0       0       0       0       0       1       0   \n",
       "2   3       0       0       0       0       0       0       0       1       0   \n",
       "3   4       1       0       0       1       6       1       5       0       0   \n",
       "4   5       0       0       0       0       0       0       0       0       0   \n",
       "\n",
       "   ...  feat_85  feat_86  feat_87  feat_88  feat_89  feat_90  feat_91  \\\n",
       "0  ...        1        0        0        0        0        0        0   \n",
       "1  ...        0        0        0        0        0        0        0   \n",
       "2  ...        0        0        0        0        0        0        0   \n",
       "3  ...        0        1        2        0        0        0        0   \n",
       "4  ...        1        0        0        0        0        1        0   \n",
       "\n",
       "   feat_92  feat_93   target  \n",
       "0        0        0  Class_1  \n",
       "1        0        0  Class_1  \n",
       "2        0        0  Class_1  \n",
       "3        0        0  Class_1  \n",
       "4        0        0  Class_1  \n",
       "\n",
       "[5 rows x 95 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#데이터 불러오기\n",
    "data = pd.read_csv('./Data/otto_train.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nid: 고유 아이디\\nfeat_1 ~ feat_93: 설명변수\\ntarget: 타겟변수 (1~9)\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "id: 고유 아이디\n",
    "feat_1 ~ feat_93: 설명변수\n",
    "target: 타겟변수 (1~9)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61878 95\n"
     ]
    }
   ],
   "source": [
    "nCar = data.shape[0] # 데이터 개수\n",
    "nVar = data.shape[1] # 변수 개수\n",
    "print(nCar,nVar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 의미 없는 변수 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(['id'], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 타겟 변수의 문자열을 숫자로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_dict = {\"Class_1\": 1,\n",
    "                \"Class_2\": 2,\n",
    "                \"Class_3\": 3,\n",
    "                \"Class_4\": 4,\n",
    "                \"Class_5\": 5,\n",
    "                \"Class_6\": 6,\n",
    "                \"Class_7\": 7,\n",
    "                \"Class_8\": 8,\n",
    "                \"Class_9\": 9}\n",
    "after_mapping_target = data['target'].apply(lambda x: mapping_dict[x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 설명변수와 타겟변수를 분리, 학습데이터와 평가데이터 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49502, 93) (12376, 93) (49502,) (12376,)\n"
     ]
    }
   ],
   "source": [
    "feature_columns = list(data.columns.difference(['target']))\n",
    "x = data[feature_columns]\n",
    "y = after_mapping_target\n",
    "train_x, test_x, train_y, test_y = train_test_split(x, y, test_size = 0.2, random_state = 42) # 학습데이터와 평가데이터의 비율을 8:2 로 분할| \n",
    "print(train_x.shape, test_x.shape, train_y.shape, test_y.shape) # 데이터 개수 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 학습 데이터를 AdaBoost 모형에 적합 후 평가 데이터로 검증"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63.23529411764706\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "tree = DecisionTreeClassifier(max_depth=5)\n",
    "AdaBoost = AdaBoostClassifier(base_estimator=tree,\n",
    "                             n_estimators=20,\n",
    "                             random_state = 42)\n",
    "model1 = AdaBoost.fit(train_x,train_y)\n",
    "pred1 = model1.predict(test_x)\n",
    "print(accuracy_score(test_y,pred1) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 더 많은 추정해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 64.14 %\n"
     ]
    }
   ],
   "source": [
    "Adaboost2 = AdaBoostClassifier(base_estimator = tree, # 트리모델을 기본으로 추정\n",
    "                                    n_estimators = 300, # 300회 추정\n",
    "                                    random_state = 42) # 시드값 고정\n",
    "model2 = Adaboost2.fit(train_x, train_y) # 학습 진행\n",
    "predict2 = model2.predict(test_x) # 평가 데이터 예측\n",
    "print(\"Accuracy: %.2f\" % (accuracy_score(test_y, predict2) * 100), \"%\") # 정확도 % 계산"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 트리의 깊이를 늘려보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 79.92 %\n"
     ]
    }
   ],
   "source": [
    "tree_model2 = DecisionTreeClassifier(max_depth = 20) # 트리 최대 깊이 20으로 새로 정의\n",
    "Adaboost_model3 = AdaBoostClassifier(base_estimator = tree_model2, # 새 트리 모델을 기본으로 추정\n",
    "                                     n_estimators = 300, # 300회 추정\n",
    "                                     random_state = 42) # 시드값 고정\n",
    "model3 = Adaboost_model3.fit(train_x, train_y) # 학습 진행\n",
    "predict3 = model3.predict(test_x) # 평가 데이터 예측\n",
    "print(\"Accuracy: %.2f\" % (accuracy_score(test_y, predict3) * 100), \"%\") # 정확도 % 계산"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 트리의 깊이를 최대로 늘려보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 70.81 %\n"
     ]
    }
   ],
   "source": [
    "tree_model3 = DecisionTreeClassifier(max_depth = 100) # 트리 최대 깊이 100으로 새로 정의\n",
    "Adaboost_model4 = AdaBoostClassifier(base_estimator = tree_model3, # 새 트리 모델을 기본으로 추정\n",
    "                                     n_estimators = 300, # 300회 추정\n",
    "                                     random_state = 42) # 시드값 고정\n",
    "model4 = Adaboost_model4.fit(train_x, train_y) # 학습 진행\n",
    "predict4 = model4.predict(test_x) # 평가 데이터 예측\n",
    "print(\"Accuracy: %.2f\" % (accuracy_score(test_y, predict4) * 100), \"%\") # 정확도 % 계산"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 학습 데이터를 XGBoost 모형에 적합 후 평가 데이터로 검증"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:50:59] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { n_estimators } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[16:51:00] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softmax' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Accuracy: 76.67 %\n",
      "Time: 7.59 seconds\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "import time\n",
    "start = time.time() #시작 시간 지정\n",
    "xgb_dtrain = xgb.DMatrix(data=train_x, label=train_y) # 학습 데이터를 XGBoost 모델에 맞게 변환\n",
    "xgb_dtest = xgb.DMatrix(data = test_x) # 평가 데이터를 XGBoost 모델에 맞게 변환\n",
    "xgb_param = {'max_depth': 10, # 트리 깊이\n",
    "         'learning_rate': 0.01, # Step Size\n",
    "         'n_estimators': 100, # Number of trees, 트리 생성 개수\n",
    "         'objective': 'multi:softmax', # 목적 함수\n",
    "        'num_class': len(set(train_y)) + 1} # 파라미터 추가, Label must be in [0, num_class) -> num_class보다 1 커야한다.\n",
    "xgb_model = xgb.train(params = xgb_param, dtrain = xgb_dtrain) # 학습 진행\n",
    "xgb_model_predict = xgb_model.predict(xgb_dtest) # 평가 데이터 예측\n",
    "print(\"Accuracy: %.2f\" % (accuracy_score(test_y, xgb_model_predict) * 100), \"%\") # 정확도 % 계산\n",
    "print(\"Time: %.2f\" % (time.time() - start), \"seconds\") # 코드 실행 시간 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5., 3., 6., ..., 9., 2., 7.], dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_model_predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 학습 데이터를 XGBoost 모형에 적합 후 평가 데이터로 검증"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:151: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.019355 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3110\n",
      "[LightGBM] [Info] Number of data points in the train set: 49502, number of used features: 93\n",
      "[LightGBM] [Info] Start training from score -34.538776\n",
      "[LightGBM] [Info] Start training from score -3.476745\n",
      "[LightGBM] [Info] Start training from score -1.341381\n",
      "[LightGBM] [Info] Start training from score -2.039019\n",
      "[LightGBM] [Info] Start training from score -3.135151\n",
      "[LightGBM] [Info] Start training from score -3.125444\n",
      "[LightGBM] [Info] Start training from score -1.481556\n",
      "[LightGBM] [Info] Start training from score -3.074772\n",
      "[LightGBM] [Info] Start training from score -1.986562\n",
      "[LightGBM] [Info] Start training from score -2.533374\n",
      "Accuracy: 76.28 %\n",
      "Time: 3.92 seconds\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "start = time.time() # 시작 시간 지정\n",
    "lgb_dtrain = lgb.Dataset(data = train_x, label = train_y) # 학습 데이터를 LightGBM 모델에 맞게 변환\n",
    "lgb_param = {'max_depth': 10, # 트리 깊이\n",
    "            'learning_rate': 0.01, # Step Size\n",
    "            'n_estimators': 100, # Number of trees, 트리 생성 개수\n",
    "            'objective': 'multiclass', # 목적 함수\n",
    "            'num_class': len(set(train_y)) + 1} # 파라미터 추가, Label must be in [0, num_class) -> num_class보다 1 커야한다.\n",
    "lgb_model = lgb.train(params = lgb_param, train_set = lgb_dtrain) # 학습 진행\n",
    "lgb_model_predict = np.argmax(lgb_model.predict(test_x), axis = 1) # 평가 데이터 예측, Softmax의 결과값 중 가장 큰 값의 Label로 예측\n",
    "print(\"Accuracy: %.2f\" % (accuracy_score(test_y, lgb_model_predict) * 100), \"%\") # 정확도 % 계산\n",
    "print(\"Time: %.2f\" % (time.time() - start), \"seconds\") # 코드 실행 시간 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.01734061e-15, 2.25081693e-02, 3.62193933e-01, ...,\n",
       "        3.24234521e-02, 5.82126692e-02, 3.67722414e-02],\n",
       "       [1.14084116e-15, 5.36978636e-02, 1.90687128e-01, ...,\n",
       "        3.25081119e-01, 9.38028846e-02, 6.50463131e-02],\n",
       "       [5.94595781e-16, 9.66842220e-03, 5.82817482e-02, ...,\n",
       "        1.42318289e-02, 3.40230275e-02, 2.14919364e-02],\n",
       "       ...,\n",
       "       [7.09105769e-16, 4.63740004e-02, 1.08297559e-01, ...,\n",
       "        5.46934960e-02, 7.24513712e-02, 5.74635996e-01],\n",
       "       [9.88127136e-16, 1.54895684e-02, 5.45515599e-01, ...,\n",
       "        2.45870954e-02, 5.65410617e-02, 3.62344513e-02],\n",
       "       [7.59617500e-16, 1.49480877e-02, 7.44570300e-02, ...,\n",
       "        5.76695793e-01, 1.43227106e-01, 2.74567219e-02]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgb_model.predict(test_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 학습 데이터를 XGBoost 모형에 적합 후 평가 데이터로 검증"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.5907034\ttotal: 646ms\tremaining: 1m 3s\n",
      "1:\tlearn: 0.6356107\ttotal: 1.34s\tremaining: 1m 5s\n",
      "2:\tlearn: 0.6411256\ttotal: 2.05s\tremaining: 1m 6s\n",
      "3:\tlearn: 0.6480344\ttotal: 2.72s\tremaining: 1m 5s\n",
      "4:\tlearn: 0.6508222\ttotal: 3.39s\tremaining: 1m 4s\n",
      "5:\tlearn: 0.6499939\ttotal: 4.13s\tremaining: 1m 4s\n",
      "6:\tlearn: 0.6507818\ttotal: 4.87s\tremaining: 1m 4s\n",
      "7:\tlearn: 0.6548422\ttotal: 5.67s\tremaining: 1m 5s\n",
      "8:\tlearn: 0.6559533\ttotal: 6.37s\tremaining: 1m 4s\n",
      "9:\tlearn: 0.6560947\ttotal: 7.08s\tremaining: 1m 3s\n",
      "10:\tlearn: 0.6568421\ttotal: 7.76s\tremaining: 1m 2s\n",
      "11:\tlearn: 0.6588219\ttotal: 8.61s\tremaining: 1m 3s\n",
      "12:\tlearn: 0.6592259\ttotal: 9.31s\tremaining: 1m 2s\n",
      "13:\tlearn: 0.6611248\ttotal: 10.1s\tremaining: 1m 1s\n",
      "14:\tlearn: 0.6625591\ttotal: 10.8s\tremaining: 1m 1s\n",
      "15:\tlearn: 0.6631853\ttotal: 11.5s\tremaining: 1m\n",
      "16:\tlearn: 0.6639328\ttotal: 12.3s\tremaining: 1m\n",
      "17:\tlearn: 0.6668821\ttotal: 13.1s\tremaining: 59.6s\n",
      "18:\tlearn: 0.6669630\ttotal: 13.8s\tremaining: 59s\n",
      "19:\tlearn: 0.6675286\ttotal: 14.7s\tremaining: 58.6s\n",
      "20:\tlearn: 0.6673266\ttotal: 15.5s\tremaining: 58.4s\n",
      "21:\tlearn: 0.6677104\ttotal: 16.3s\tremaining: 57.9s\n",
      "22:\tlearn: 0.6682558\ttotal: 17.1s\tremaining: 57.2s\n",
      "23:\tlearn: 0.6683972\ttotal: 17.8s\tremaining: 56.3s\n",
      "24:\tlearn: 0.6686599\ttotal: 18.5s\tremaining: 55.5s\n",
      "25:\tlearn: 0.6681952\ttotal: 19.2s\tremaining: 54.7s\n",
      "26:\tlearn: 0.6684982\ttotal: 20s\tremaining: 54s\n",
      "27:\tlearn: 0.6692053\ttotal: 20.7s\tremaining: 53.2s\n",
      "28:\tlearn: 0.6696699\ttotal: 21.4s\tremaining: 52.5s\n",
      "29:\tlearn: 0.6699325\ttotal: 22.2s\tremaining: 51.8s\n",
      "30:\tlearn: 0.6705992\ttotal: 22.9s\tremaining: 51s\n",
      "31:\tlearn: 0.6709426\ttotal: 23.7s\tremaining: 50.3s\n",
      "32:\tlearn: 0.6708012\ttotal: 24.4s\tremaining: 49.5s\n",
      "33:\tlearn: 0.6709426\ttotal: 25.1s\tremaining: 48.7s\n",
      "34:\tlearn: 0.6707002\ttotal: 25.9s\tremaining: 48s\n",
      "35:\tlearn: 0.6715082\ttotal: 26.6s\tremaining: 47.2s\n",
      "36:\tlearn: 0.6705992\ttotal: 27.3s\tremaining: 46.5s\n",
      "37:\tlearn: 0.6725991\ttotal: 28s\tremaining: 45.7s\n",
      "38:\tlearn: 0.6729829\ttotal: 28.7s\tremaining: 45s\n",
      "39:\tlearn: 0.6725991\ttotal: 29.5s\tremaining: 44.2s\n",
      "40:\tlearn: 0.6734273\ttotal: 30.2s\tremaining: 43.4s\n",
      "41:\tlearn: 0.6738314\ttotal: 31s\tremaining: 42.8s\n",
      "42:\tlearn: 0.6741546\ttotal: 31.8s\tremaining: 42.1s\n",
      "43:\tlearn: 0.6739728\ttotal: 32.7s\tremaining: 41.6s\n",
      "44:\tlearn: 0.6741950\ttotal: 33.5s\tremaining: 41s\n",
      "45:\tlearn: 0.6750636\ttotal: 34.3s\tremaining: 40.2s\n",
      "46:\tlearn: 0.6758919\ttotal: 35s\tremaining: 39.5s\n",
      "47:\tlearn: 0.6757707\ttotal: 35.8s\tremaining: 38.8s\n",
      "48:\tlearn: 0.6762151\ttotal: 36.6s\tremaining: 38.1s\n",
      "49:\tlearn: 0.6774474\ttotal: 37.3s\tremaining: 37.3s\n",
      "50:\tlearn: 0.6777100\ttotal: 38.1s\tremaining: 36.6s\n",
      "51:\tlearn: 0.6786594\ttotal: 38.8s\tremaining: 35.8s\n",
      "52:\tlearn: 0.6789827\ttotal: 39.6s\tremaining: 35.1s\n",
      "53:\tlearn: 0.6804372\ttotal: 40.3s\tremaining: 34.4s\n",
      "54:\tlearn: 0.6804372\ttotal: 41.1s\tremaining: 33.6s\n",
      "55:\tlearn: 0.6809220\ttotal: 41.8s\tremaining: 32.8s\n",
      "56:\tlearn: 0.6812250\ttotal: 42.5s\tremaining: 32.1s\n",
      "57:\tlearn: 0.6813058\ttotal: 43.2s\tremaining: 31.3s\n",
      "58:\tlearn: 0.6811846\ttotal: 43.9s\tremaining: 30.5s\n",
      "59:\tlearn: 0.6813260\ttotal: 44.7s\tremaining: 29.8s\n",
      "60:\tlearn: 0.6816694\ttotal: 45.4s\tremaining: 29s\n",
      "61:\tlearn: 0.6823159\ttotal: 46.2s\tremaining: 28.3s\n",
      "62:\tlearn: 0.6832653\ttotal: 47s\tremaining: 27.6s\n",
      "63:\tlearn: 0.6840734\ttotal: 47.9s\tremaining: 26.9s\n",
      "64:\tlearn: 0.6840734\ttotal: 48.7s\tremaining: 26.2s\n",
      "65:\tlearn: 0.6846592\ttotal: 49.5s\tremaining: 25.5s\n",
      "66:\tlearn: 0.6843360\ttotal: 50.3s\tremaining: 24.8s\n",
      "67:\tlearn: 0.6846390\ttotal: 51s\tremaining: 24s\n",
      "68:\tlearn: 0.6854269\ttotal: 51.8s\tremaining: 23.3s\n",
      "69:\tlearn: 0.6858309\ttotal: 52.5s\tremaining: 22.5s\n",
      "70:\tlearn: 0.6858309\ttotal: 53.2s\tremaining: 21.7s\n",
      "71:\tlearn: 0.6865783\ttotal: 54s\tremaining: 21s\n",
      "72:\tlearn: 0.6864167\ttotal: 54.7s\tremaining: 20.2s\n",
      "73:\tlearn: 0.6868611\ttotal: 55.5s\tremaining: 19.5s\n",
      "74:\tlearn: 0.6869217\ttotal: 56.2s\tremaining: 18.7s\n",
      "75:\tlearn: 0.6870429\ttotal: 56.9s\tremaining: 18s\n",
      "76:\tlearn: 0.6875278\ttotal: 57.7s\tremaining: 17.2s\n",
      "77:\tlearn: 0.6881136\ttotal: 58.4s\tremaining: 16.5s\n",
      "78:\tlearn: 0.6883762\ttotal: 59.2s\tremaining: 15.7s\n",
      "79:\tlearn: 0.6888207\ttotal: 60s\tremaining: 15s\n",
      "80:\tlearn: 0.6892449\ttotal: 1m\tremaining: 14.3s\n",
      "81:\tlearn: 0.6898509\ttotal: 1m 1s\tremaining: 13.5s\n",
      "82:\tlearn: 0.6897095\ttotal: 1m 2s\tremaining: 12.8s\n",
      "83:\tlearn: 0.6902549\ttotal: 1m 3s\tremaining: 12s\n",
      "84:\tlearn: 0.6909822\ttotal: 1m 4s\tremaining: 11.3s\n",
      "85:\tlearn: 0.6910832\ttotal: 1m 4s\tremaining: 10.6s\n",
      "86:\tlearn: 0.6914468\ttotal: 1m 5s\tremaining: 9.79s\n",
      "87:\tlearn: 0.6916084\ttotal: 1m 6s\tremaining: 9.04s\n",
      "88:\tlearn: 0.6919922\ttotal: 1m 7s\tremaining: 8.28s\n",
      "89:\tlearn: 0.6925579\ttotal: 1m 7s\tremaining: 7.53s\n",
      "90:\tlearn: 0.6928407\ttotal: 1m 8s\tremaining: 6.78s\n",
      "91:\tlearn: 0.6930427\ttotal: 1m 9s\tremaining: 6.03s\n",
      "92:\tlearn: 0.6935073\ttotal: 1m 10s\tremaining: 5.27s\n",
      "93:\tlearn: 0.6940932\ttotal: 1m 10s\tremaining: 4.52s\n",
      "94:\tlearn: 0.6944972\ttotal: 1m 11s\tremaining: 3.76s\n",
      "95:\tlearn: 0.6948810\ttotal: 1m 12s\tremaining: 3.01s\n",
      "96:\tlearn: 0.6951840\ttotal: 1m 13s\tremaining: 2.26s\n",
      "97:\tlearn: 0.6954264\ttotal: 1m 13s\tremaining: 1.5s\n",
      "98:\tlearn: 0.6955881\ttotal: 1m 14s\tremaining: 752ms\n",
      "99:\tlearn: 0.6956285\ttotal: 1m 15s\tremaining: 0us\n",
      "Accuracy: 69.64 %\n",
      "Time: 75.67 seconds\n"
     ]
    }
   ],
   "source": [
    "import catboost as cb\n",
    "start = time.time() # 시작 시간 지정\n",
    "cb_dtrain = cb.Pool(data = train_x, label = train_y) # 학습 데이터를 Catboost 모델에 맞게 변환\n",
    "cb_param = {'max_depth': 10, # 트리 깊이\n",
    "            'learning_rate': 0.01, # Step Size\n",
    "            'n_estimators': 100, # Number of trees, 트리 생성 개수\n",
    "            'eval_metric': 'Accuracy', # 평가 척도\n",
    "            'loss_function': 'MultiClass'} # 손실 함수, 목적 함수\n",
    "cb_model = cb.train(pool = cb_dtrain, params = cb_param) # 학습 진행\n",
    "cb_model_predict = np.argmax(cb_model.predict(test_x), axis = 1) + 1 # 평가 데이터 예측, Softmax의 결과값 중 가장 큰 값의 Label로 예측, 인덱스의 순서를 맞추기 위해 +1\n",
    "print(\"Accuracy: %.2f\" % (accuracy_score(test_y, cb_model_predict) * 100), \"%\") # 정확도 % 계산\n",
    "print(\"Time: %.2f\" % (time.time() - start), \"seconds\") # 코드 실행 시간 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.35426047,  1.22109587,  0.44230101, ..., -0.1698448 ,\n",
       "        -0.02059177, -0.2130643 ],\n",
       "       [-0.07235138,  0.42535181,  0.20060428, ...,  0.21863604,\n",
       "         0.2719157 ,  0.25089315],\n",
       "       [-0.3315885 , -0.31862353, -0.31279765, ..., -0.29798357,\n",
       "        -0.24018767, -0.32984969],\n",
       "       ...,\n",
       "       [ 0.05304325,  0.02500267, -0.14752573, ..., -0.20741963,\n",
       "         0.12789417,  1.51166757],\n",
       "       [-0.55093666,  1.7691278 ,  0.99746884, ..., -0.3420542 ,\n",
       "        -0.49799871, -0.38136323],\n",
       "       [-0.3033724 ,  0.09352675, -0.11808658, ...,  0.65825036,\n",
       "         1.05515787, -0.20799899]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cb_model.predict(test_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 새로운 데이터로 앙상블 모델 해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>price</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>floors</th>\n",
       "      <th>waterfront</th>\n",
       "      <th>condition</th>\n",
       "      <th>grade</th>\n",
       "      <th>yr_built</th>\n",
       "      <th>yr_renovated</th>\n",
       "      <th>zipcode</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7129300520</td>\n",
       "      <td>20141013T000000</td>\n",
       "      <td>221900.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>1955</td>\n",
       "      <td>0</td>\n",
       "      <td>98178</td>\n",
       "      <td>47.5112</td>\n",
       "      <td>-122.257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6414100192</td>\n",
       "      <td>20141209T000000</td>\n",
       "      <td>538000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.25</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>1951</td>\n",
       "      <td>1991</td>\n",
       "      <td>98125</td>\n",
       "      <td>47.7210</td>\n",
       "      <td>-122.319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5631500400</td>\n",
       "      <td>20150225T000000</td>\n",
       "      <td>180000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>1933</td>\n",
       "      <td>0</td>\n",
       "      <td>98028</td>\n",
       "      <td>47.7379</td>\n",
       "      <td>-122.233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2487200875</td>\n",
       "      <td>20141209T000000</td>\n",
       "      <td>604000.0</td>\n",
       "      <td>4</td>\n",
       "      <td>3.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>1965</td>\n",
       "      <td>0</td>\n",
       "      <td>98136</td>\n",
       "      <td>47.5208</td>\n",
       "      <td>-122.393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1954400510</td>\n",
       "      <td>20150218T000000</td>\n",
       "      <td>510000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>1987</td>\n",
       "      <td>0</td>\n",
       "      <td>98074</td>\n",
       "      <td>47.6168</td>\n",
       "      <td>-122.045</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id             date     price  bedrooms  bathrooms  floors  \\\n",
       "0  7129300520  20141013T000000  221900.0         3       1.00     1.0   \n",
       "1  6414100192  20141209T000000  538000.0         3       2.25     2.0   \n",
       "2  5631500400  20150225T000000  180000.0         2       1.00     1.0   \n",
       "3  2487200875  20141209T000000  604000.0         4       3.00     1.0   \n",
       "4  1954400510  20150218T000000  510000.0         3       2.00     1.0   \n",
       "\n",
       "   waterfront  condition  grade  yr_built  yr_renovated  zipcode      lat  \\\n",
       "0           0          3      7      1955             0    98178  47.5112   \n",
       "1           0          3      7      1951          1991    98125  47.7210   \n",
       "2           0          3      6      1933             0    98028  47.7379   \n",
       "3           0          5      7      1965             0    98136  47.5208   \n",
       "4           0          3      8      1987             0    98074  47.6168   \n",
       "\n",
       "      long  \n",
       "0 -122.257  \n",
       "1 -122.319  \n",
       "2 -122.233  \n",
       "3 -122.393  \n",
       "4 -122.045  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터 불러오기\n",
    "data = pd.read_csv(\"./Data/kc_house_data.csv\") \n",
    "data.head() # 데이터 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(['id', 'date', 'zipcode', 'lat', 'long'], axis = 1) # id, date, zipcode, lat, long  제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15129, 8) (6484, 8) (15129,) (6484,)\n"
     ]
    }
   ],
   "source": [
    "feature_columns = list(data.columns.difference(['price'])) # Price를 제외한 모든 행\n",
    "x = data[feature_columns]\n",
    "y = data['price']\n",
    "train_x, test_x, train_y, test_y = train_test_split(x, y, test_size = 0.3, random_state = 42) # 학습데이터와 평가데이터의 비율을 7:3\n",
    "print(train_x.shape, test_x.shape, train_y.shape, test_y.shape) # 데이터 개수 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:151: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000899 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 237\n",
      "[LightGBM] [Info] Number of data points in the train set: 15129, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 537729.263666\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "start = time.time() # 시작 시간 지정\n",
    "lgb_dtrain = lgb.Dataset(data = train_x, label = train_y) # 학습 데이터를 LightGBM 모델에 맞게 변환\n",
    "lgb_param = {'max_depth': 10, # 트리 깊이\n",
    "            'learning_rate': 0.01, # Step Size\n",
    "            'n_estimators': 500, # Number of trees, 트리 생성 개수\n",
    "            'objective': 'regression'} # 파라미터 추가, Label must be in [0, num_class) -> num_class보다 1 커야한다.\n",
    "lgb_model = lgb.train(params = lgb_param, train_set = lgb_dtrain) # 학습 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "210904.17249451784"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from math import sqrt\n",
    "\n",
    "sqrt(mean_squared_error(lgb_model.predict(test_x),test_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble의 Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9586\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000403 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 236\n",
      "[LightGBM] [Info] Number of data points in the train set: 15129, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 536489.340274\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:151: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9533\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000297 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 233\n",
      "[LightGBM] [Info] Number of data points in the train set: 15129, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 536186.398374\n",
      "9561\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000338 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 233\n",
      "[LightGBM] [Info] Number of data points in the train set: 15129, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 537924.224536\n",
      "9556\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000376 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 236\n",
      "[LightGBM] [Info] Number of data points in the train set: 15129, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 537944.965100\n",
      "9559\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000420 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 231\n",
      "[LightGBM] [Info] Number of data points in the train set: 15129, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 535101.619671\n",
      "9552\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000302 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 229\n",
      "[LightGBM] [Info] Number of data points in the train set: 15129, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 538413.785776\n",
      "9605\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000863 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 232\n",
      "[LightGBM] [Info] Number of data points in the train set: 15129, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 535402.532091\n",
      "9589\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000419 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 231\n",
      "[LightGBM] [Info] Number of data points in the train set: 15129, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 534425.722454\n",
      "9548\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000861 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 230\n",
      "[LightGBM] [Info] Number of data points in the train set: 15129, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 540266.309538\n",
      "9602\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000542 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 235\n",
      "[LightGBM] [Info] Number of data points in the train set: 15129, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 534767.494679\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "bagging_predict_result = [] # 빈 리스트 생성\n",
    "for _ in range(10):\n",
    "    data_index = [data_index for data_index in range(train_x.shape[0])] # 학습 데이터의 인덱스를 리스트로 변환\n",
    "    random_data_index = np.random.choice(data_index, train_x.shape[0]) # 데이터의 1/10 크기만큼 랜덤 샘플링, // 는 소수점을 무시하기 위함\n",
    "    print(len(set(random_data_index)))\n",
    "    lgb_dtrain = lgb.Dataset(data = train_x.iloc[random_data_index,], label = train_y.iloc[random_data_index,]) # 학습 데이터를 LightGBM 모델에 맞게 변환\n",
    "    lgb_param = {'max_depth': 14, # 트리 깊이\n",
    "            'learning_rate': 0.01, # Step Size\n",
    "            'n_estimators': 500, # Number of trees, 트리 생성 개수\n",
    "            'objective': 'regression'} # 파라미터 추가, Label must be in [0, num_class) -> num_class보다 1 커야한다.\n",
    "    lgb_model = lgb.train(params = lgb_param, train_set = lgb_dtrain) # 학습 진행\n",
    "    predict1 = lgb_model.predict(test_x) # 테스트 데이터 예측\n",
    "    bagging_predict_result.append(predict1) # 반복문이 실행되기 전 빈 리스트에 결과 값 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([507253.3866251 , 659586.02376282, 921778.26051183, ...,\n",
       "        363994.64941803, 907335.59599342, 453267.07203624]),\n",
       " array([ 523041.42667995,  624390.04819792, 1035112.3486997 , ...,\n",
       "         335089.07638276,  941202.27335475,  456945.59804323]),\n",
       " array([501082.07330118, 636448.01797546, 962702.29818413, ...,\n",
       "        339826.90760119, 920179.88267951, 457641.25234293]),\n",
       " array([ 490709.93003947,  600085.42712056, 1018997.81337543, ...,\n",
       "         339579.91144283,  901906.87483598,  455147.97379538]),\n",
       " array([514404.76663979, 656126.16586637, 906849.60459994, ...,\n",
       "        319866.88132659, 915834.25849097, 464921.99126374]),\n",
       " array([487920.42338702, 639920.64526923, 954732.44311262, ...,\n",
       "        348247.87685872, 989063.52285168, 464428.38884282]),\n",
       " array([492004.80111638, 614784.29377639, 900416.20156853, ...,\n",
       "        336711.88827712, 946207.34003005, 486722.98770536]),\n",
       " array([496229.92496608, 587476.60824038, 912643.79691362, ...,\n",
       "        349647.02515463, 893255.23710104, 462204.55286347]),\n",
       " array([501179.24795411, 608204.9609509 , 988645.48972708, ...,\n",
       "        328494.30618633, 971321.00619234, 442481.39286894]),\n",
       " array([487184.2949503 , 625496.20761331, 934192.96615226, ...,\n",
       "        342420.48907649, 938637.84879042, 467554.58072339])]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bagging_predict_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bagging을 바탕으로 예측한 결과값에 대한 평균을 계산\n",
    "bagging_predict = [] # 빈 리스트 생성\n",
    "for lst2_index in range(test_x.shape[0]): # 테스트 데이터 개수만큼의 반복\n",
    "    temp_predict = [] # 임시 빈 리스트 생성 (반복문 내 결과값 저장)\n",
    "    for lst_index in range(len(bagging_predict_result)): # Bagging 결과 리스트 반복\n",
    "        temp_predict.append(bagging_predict_result[lst_index][lst2_index]) # 각 Bagging 결과 예측한 값 중 같은 인덱스를 리스트에 저장\n",
    "    bagging_predict.append(np.mean(temp_predict)) # 해당 인덱스의 30개의 결과값에 대한 평균을 최종 리스트에 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 211204.12925643113\n"
     ]
    }
   ],
   "source": [
    "# 예측한 결과값들의 평균을 계산하여 실제 테스트 데이트의 타겟변수와 비교하여 성능 평가\n",
    "\n",
    "print(\"RMSE: {}\".format(sqrt(mean_squared_error(bagging_predict, test_y)))) # RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[500101.0275659395,\n",
       " 625251.8398773337,\n",
       " 953607.1222845137,\n",
       " 1625448.880721765,\n",
       " 637409.6677421046,\n",
       " 368633.7881387803,\n",
       " 697387.7101177999,\n",
       " 433982.1799216671,\n",
       " 464701.00663997914,\n",
       " 498286.4722826725,\n",
       " 645024.2467170774,\n",
       " 383204.3377799808,\n",
       " 298688.99246290093,\n",
       " 359742.64434098726,\n",
       " 339211.33049235353,\n",
       " 1265932.934480607,\n",
       " 373711.9681859823,\n",
       " 1046176.8498400789,\n",
       " 317608.0474228215,\n",
       " 523011.33408646256,\n",
       " 375594.0518530584,\n",
       " 1810688.1934308554,\n",
       " 664327.6408161392,\n",
       " 533051.7488045788,\n",
       " 507620.35926165135,\n",
       " 483741.8048304993,\n",
       " 294410.82522664394,\n",
       " 252533.02583555473,\n",
       " 477728.5215858297,\n",
       " 535181.3549899565,\n",
       " 495618.0699986238,\n",
       " 470948.06113383325,\n",
       " 462634.15734258585,\n",
       " 584360.1335150127,\n",
       " 375504.63316235127,\n",
       " 1034013.6669728311,\n",
       " 910581.3145368124,\n",
       " 524468.7119506458,\n",
       " 353391.56530882284,\n",
       " 1547420.642368413,\n",
       " 387137.0286535895,\n",
       " 277367.7831772934,\n",
       " 509911.00167048787,\n",
       " 342036.30631919584,\n",
       " 253555.26528125428,\n",
       " 248137.0870182556,\n",
       " 332012.1862137965,\n",
       " 332768.0448444452,\n",
       " 352033.81930312794,\n",
       " 569289.3236821456,\n",
       " 371654.15529137355,\n",
       " 342350.65172738594,\n",
       " 761181.0106919978,\n",
       " 336772.825753931,\n",
       " 468610.7481596296,\n",
       " 1687502.6147839061,\n",
       " 471878.2942677697,\n",
       " 707705.1390994391,\n",
       " 329360.7650330056,\n",
       " 650745.126674524,\n",
       " 482833.88588594564,\n",
       " 373630.6483950763,\n",
       " 299532.6485749835,\n",
       " 520961.31509418663,\n",
       " 448052.4380555819,\n",
       " 283600.910173969,\n",
       " 389611.8318205408,\n",
       " 1629172.2050684732,\n",
       " 484653.7631239838,\n",
       " 657523.866727101,\n",
       " 433544.0788509868,\n",
       " 299012.0378474527,\n",
       " 772712.3850220949,\n",
       " 511693.8984120288,\n",
       " 506924.94899507816,\n",
       " 1247163.778966084,\n",
       " 807068.7242422197,\n",
       " 288875.20418363914,\n",
       " 455078.41963425756,\n",
       " 928780.6026613569,\n",
       " 639840.037857526,\n",
       " 376187.46751510433,\n",
       " 655168.4032835511,\n",
       " 359695.7126862057,\n",
       " 828228.234053808,\n",
       " 518240.0847254427,\n",
       " 522166.7714165547,\n",
       " 552938.9483306485,\n",
       " 359132.41825219936,\n",
       " 474015.3599270276,\n",
       " 351097.3390021523,\n",
       " 398247.50779006403,\n",
       " 631371.4644700994,\n",
       " 1080528.2055517365,\n",
       " 435627.27085360454,\n",
       " 494698.3809671317,\n",
       " 361100.51706436044,\n",
       " 303104.6289919867,\n",
       " 815145.829582975,\n",
       " 456557.6789517924,\n",
       " 257781.9066654134,\n",
       " 904707.4962279644,\n",
       " 963013.4794530496,\n",
       " 482437.04413565516,\n",
       " 1052337.6787275819,\n",
       " 298494.42295180727,\n",
       " 491360.3939736172,\n",
       " 478370.0387298415,\n",
       " 812251.39625502,\n",
       " 2287233.7225709455,\n",
       " 550314.7795775151,\n",
       " 327132.7278260001,\n",
       " 555012.8690724726,\n",
       " 622803.06349432,\n",
       " 548760.8576908676,\n",
       " 335216.7323921524,\n",
       " 314048.9247621837,\n",
       " 256504.75095560742,\n",
       " 330916.18539933854,\n",
       " 339211.33049235353,\n",
       " 378437.2903417717,\n",
       " 284039.8388357966,\n",
       " 344052.1491712638,\n",
       " 258402.72185568613,\n",
       " 597312.2500722138,\n",
       " 653420.3408125923,\n",
       " 274348.2168208468,\n",
       " 727593.4433785549,\n",
       " 455390.58246536163,\n",
       " 424793.4163162975,\n",
       " 528821.3663452866,\n",
       " 467335.0804526287,\n",
       " 400483.677454761,\n",
       " 832186.0330828258,\n",
       " 376492.4263252895,\n",
       " 459523.3046013401,\n",
       " 383362.7800266467,\n",
       " 352799.87946418085,\n",
       " 928746.8909835884,\n",
       " 616444.0013211285,\n",
       " 519899.2731024556,\n",
       " 781669.0831477452,\n",
       " 892193.8654645134,\n",
       " 406232.4759858521,\n",
       " 261396.63567895,\n",
       " 393255.99811965734,\n",
       " 476291.4558084385,\n",
       " 251553.50964736225,\n",
       " 417464.91232614126,\n",
       " 474320.7700302036,\n",
       " 573248.6235268402,\n",
       " 684479.0878167839,\n",
       " 540439.790068215,\n",
       " 1123981.03700122,\n",
       " 937401.8139591146,\n",
       " 875554.461003465,\n",
       " 575127.2456501296,\n",
       " 654005.2151891945,\n",
       " 584562.1335023562,\n",
       " 492774.9164882259,\n",
       " 644324.7090588235,\n",
       " 366810.36633805226,\n",
       " 332768.0448444452,\n",
       " 357527.29224841937,\n",
       " 361856.2091276878,\n",
       " 345046.83568327327,\n",
       " 286404.0393544432,\n",
       " 314202.21524850314,\n",
       " 450533.02977385157,\n",
       " 461822.6473980689,\n",
       " 622095.452495831,\n",
       " 395743.2404081232,\n",
       " 465837.6908274592,\n",
       " 584077.2316191535,\n",
       " 427153.66690019163,\n",
       " 406150.3990524863,\n",
       " 361454.5919855197,\n",
       " 672197.8841426417,\n",
       " 346853.9900534235,\n",
       " 256011.71965890037,\n",
       " 314563.2626088294,\n",
       " 478768.4290452356,\n",
       " 530014.8317105068,\n",
       " 661128.1569089087,\n",
       " 464977.9741199926,\n",
       " 471155.07565981336,\n",
       " 272005.2637793778,\n",
       " 425384.7927681661,\n",
       " 352401.1013262016,\n",
       " 347561.86876588035,\n",
       " 373419.8629209973,\n",
       " 652528.5820201902,\n",
       " 1577715.7069802838,\n",
       " 1233804.0621909683,\n",
       " 266297.07246033306,\n",
       " 489229.2903131405,\n",
       " 495678.2780150975,\n",
       " 1617811.9933406138,\n",
       " 462207.6646961239,\n",
       " 463880.3103361338,\n",
       " 318396.7633365068,\n",
       " 383555.0593715973,\n",
       " 522588.3120287831,\n",
       " 746546.6268164434,\n",
       " 790938.7225317236,\n",
       " 306576.05548394786,\n",
       " 507620.35926165135,\n",
       " 309973.6504509336,\n",
       " 507163.90148419654,\n",
       " 1449764.6759661573,\n",
       " 361100.51706436044,\n",
       " 416428.07604992285,\n",
       " 456219.7307746308,\n",
       " 361100.51706436044,\n",
       " 334853.66729323287,\n",
       " 709435.3351961949,\n",
       " 791201.2486791739,\n",
       " 345642.5989815235,\n",
       " 363535.56613525783,\n",
       " 360954.6912586733,\n",
       " 1683746.6977475192,\n",
       " 534748.0576357677,\n",
       " 504996.9686669906,\n",
       " 445795.6107436878,\n",
       " 522207.7351423901,\n",
       " 740740.6829917475,\n",
       " 344805.938306864,\n",
       " 1241739.4984108056,\n",
       " 898278.7612681495,\n",
       " 464185.8815864875,\n",
       " 355711.5113731063,\n",
       " 469443.5809039652,\n",
       " 720159.607924915,\n",
       " 298190.0474468443,\n",
       " 348938.21945493354,\n",
       " 388122.67260999716,\n",
       " 351930.4984929549,\n",
       " 356291.7800924804,\n",
       " 2485432.9139429056,\n",
       " 338788.8372720834,\n",
       " 446244.6675833744,\n",
       " 463404.94074239087,\n",
       " 595116.8056004115,\n",
       " 412719.64167359285,\n",
       " 471382.1835870997,\n",
       " 309612.21865193674,\n",
       " 511386.7966361499,\n",
       " 558930.7152384343,\n",
       " 719370.6185616341,\n",
       " 850194.5556090085,\n",
       " 526513.7055391708,\n",
       " 428434.93500793754,\n",
       " 735299.528502916,\n",
       " 355669.6540493261,\n",
       " 347561.86876588035,\n",
       " 528995.5309951932,\n",
       " 507596.3053223477,\n",
       " 468281.6537862988,\n",
       " 896723.4958831349,\n",
       " 372359.63222407707,\n",
       " 3286602.8132190765,\n",
       " 629877.8589241386,\n",
       " 757213.7626559521,\n",
       " 1059352.2578521832,\n",
       " 504931.092402178,\n",
       " 673679.4480790118,\n",
       " 923195.4269830033,\n",
       " 352807.1301530056,\n",
       " 720912.6638365183,\n",
       " 440049.7562129063,\n",
       " 471207.0582573478,\n",
       " 344248.7324855579,\n",
       " 287276.667965842,\n",
       " 437748.91085274785,\n",
       " 412857.16575907817,\n",
       " 1402256.6812528796,\n",
       " 306531.45076853305,\n",
       " 342972.2071948885,\n",
       " 530014.5729849346,\n",
       " 362019.70369485376,\n",
       " 309606.3445493216,\n",
       " 509516.8176822577,\n",
       " 368852.58907590644,\n",
       " 443706.38086424733,\n",
       " 488814.96805572446,\n",
       " 445279.9573540352,\n",
       " 372743.89403026784,\n",
       " 637712.4172295706,\n",
       " 355767.0971091903,\n",
       " 315846.03710489575,\n",
       " 780567.1179354633,\n",
       " 438835.9708576696,\n",
       " 262227.55866028037,\n",
       " 349489.96040814265,\n",
       " 652528.5820201902,\n",
       " 649698.3970010481,\n",
       " 493477.63921960874,\n",
       " 444712.00226640876,\n",
       " 453853.1975505436,\n",
       " 574214.1382841296,\n",
       " 473690.7189905195,\n",
       " 545684.7568417531,\n",
       " 328365.138178932,\n",
       " 568354.8381341181,\n",
       " 344154.8762180345,\n",
       " 786919.439606161,\n",
       " 450533.02977385157,\n",
       " 387826.1105947393,\n",
       " 333901.6983661469,\n",
       " 326882.22861765086,\n",
       " 365597.41455893335,\n",
       " 290268.17573778593,\n",
       " 844749.2601785755,\n",
       " 1576145.3873710267,\n",
       " 947289.0219382495,\n",
       " 448174.9223158391,\n",
       " 806066.2255701706,\n",
       " 464498.3040620579,\n",
       " 812044.9920970591,\n",
       " 344506.0275483585,\n",
       " 401945.6839855532,\n",
       " 494268.2452797784,\n",
       " 266297.07246033306,\n",
       " 301405.00615085475,\n",
       " 457023.2605703787,\n",
       " 461822.6473980689,\n",
       " 569973.3827854666,\n",
       " 298654.24621414597,\n",
       " 510891.34332255146,\n",
       " 258402.72185568613,\n",
       " 666310.2150261959,\n",
       " 295586.8948680244,\n",
       " 497743.89778103156,\n",
       " 299581.4400815405,\n",
       " 394907.5424673695,\n",
       " 484403.71602517384,\n",
       " 618085.5494289725,\n",
       " 462503.2582263332,\n",
       " 916549.7843275452,\n",
       " 258095.20064273328,\n",
       " 1670965.6587776581,\n",
       " 474274.5862380299,\n",
       " 436592.28892243124,\n",
       " 575633.350986906,\n",
       " 672352.4774643778,\n",
       " 619424.2160049949,\n",
       " 342948.5511829798,\n",
       " 463111.3922674373,\n",
       " 472473.72228975437,\n",
       " 715125.0876574209,\n",
       " 283594.1730652425,\n",
       " 363087.0715777271,\n",
       " 477073.92822283367,\n",
       " 605698.0708818954,\n",
       " 340674.045145124,\n",
       " 838195.847503708,\n",
       " 560053.9603666053,\n",
       " 897143.5038059553,\n",
       " 998782.3909195152,\n",
       " 633352.904275764,\n",
       " 372688.33123770903,\n",
       " 803854.6574323,\n",
       " 353890.71681940806,\n",
       " 568884.833573878,\n",
       " 682074.812435472,\n",
       " 325760.16412089684,\n",
       " 771482.1348954111,\n",
       " 401422.45811771054,\n",
       " 938560.7030438269,\n",
       " 382978.83051967644,\n",
       " 502807.1607820696,\n",
       " 672495.6900180067,\n",
       " 581233.2032701735,\n",
       " 349022.77769245364,\n",
       " 545226.2948365749,\n",
       " 375404.6143673026,\n",
       " 337793.1289714877,\n",
       " 543301.1252639901,\n",
       " 387813.52614113956,\n",
       " 417504.91594835074,\n",
       " 374906.54796706385,\n",
       " 714869.0384952116,\n",
       " 630452.448058122,\n",
       " 434235.07061496086,\n",
       " 464022.39536839677,\n",
       " 458856.49284188746,\n",
       " 299071.2262454033,\n",
       " 489022.02741496207,\n",
       " 427577.4074181857,\n",
       " 468541.3517670425,\n",
       " 552052.7286147125,\n",
       " 385080.05588450807,\n",
       " 377454.69910179323,\n",
       " 406071.9427218138,\n",
       " 1411954.2943487444,\n",
       " 382978.83051967644,\n",
       " 344506.0275483585,\n",
       " 1163765.8956683998,\n",
       " 667112.6905394935,\n",
       " 383825.8106259521,\n",
       " 433544.0788509868,\n",
       " 478209.1815319619,\n",
       " 639623.833091536,\n",
       " 456648.0192669469,\n",
       " 401846.93013763725,\n",
       " 430687.15879517456,\n",
       " 461822.6473980689,\n",
       " 424793.4163162975,\n",
       " 487493.0896934895,\n",
       " 471142.8144789544,\n",
       " 347813.26585012005,\n",
       " 474175.6313053776,\n",
       " 606389.2021389101,\n",
       " 421178.6545534039,\n",
       " 271247.3978011093,\n",
       " 380356.2876063649,\n",
       " 465016.20377227105,\n",
       " 452018.00981201493,\n",
       " 1060783.4617483227,\n",
       " 454948.35015236196,\n",
       " 398755.52068924933,\n",
       " 554084.0441858419,\n",
       " 332392.9629251878,\n",
       " 674604.1723983313,\n",
       " 405737.72192597535,\n",
       " 465579.15501109854,\n",
       " 487854.5379276135,\n",
       " 470786.99586080294,\n",
       " 553399.9330004216,\n",
       " 334164.8247278965,\n",
       " 615864.357827767,\n",
       " 506678.95975510345,\n",
       " 710897.5265304421,\n",
       " 532497.1628224507,\n",
       " 512631.63515789446,\n",
       " 523596.4179173809,\n",
       " 516329.7439185677,\n",
       " 384221.36527323397,\n",
       " 360197.9269965916,\n",
       " 353100.7381679723,\n",
       " 644789.3860138756,\n",
       " 381337.42961180554,\n",
       " 356435.332254341,\n",
       " 276501.1323179471,\n",
       " 367265.08040727506,\n",
       " 383483.42698355246,\n",
       " 832186.0330828258,\n",
       " 2254022.949642896,\n",
       " 444073.49600400095,\n",
       " 1161120.7407465356,\n",
       " 501698.9685189224,\n",
       " 269362.20040986885,\n",
       " 484460.0783344616,\n",
       " 450570.5139052634,\n",
       " 438234.2654253392,\n",
       " 662389.257062097,\n",
       " 378046.19467399124,\n",
       " 818079.3437594008,\n",
       " 375341.06305069453,\n",
       " 273033.62015346123,\n",
       " 468567.37885240576,\n",
       " 635329.1898944654,\n",
       " 963866.4036367843,\n",
       " 375500.9107765891,\n",
       " 730189.5477852243,\n",
       " 452437.6137760562,\n",
       " 357540.08935813635,\n",
       " 537615.2395374223,\n",
       " 941736.8585929312,\n",
       " 436564.06061590865,\n",
       " 462503.2582263332,\n",
       " 347813.26585012005,\n",
       " 455796.6537303209,\n",
       " 1245160.680211695,\n",
       " 447213.40729332826,\n",
       " 497007.51806030795,\n",
       " 387900.2121693505,\n",
       " 471770.4247770205,\n",
       " 317602.91033380537,\n",
       " 452340.82833697664,\n",
       " 304474.62359877944,\n",
       " 335657.68144710007,\n",
       " 347134.28701432835,\n",
       " 390503.5726183618,\n",
       " 461077.8436272674,\n",
       " 631997.4587525671,\n",
       " 510993.00950380845,\n",
       " 335251.9780541452,\n",
       " 746519.8392679399,\n",
       " 657214.8278752228,\n",
       " 688333.3821748791,\n",
       " 350477.94563501736,\n",
       " 427153.66690019163,\n",
       " 655683.9375061996,\n",
       " 1696596.8115546051,\n",
       " 495287.9655688639,\n",
       " 689464.8650030451,\n",
       " 370990.05330787075,\n",
       " 370770.78748756106,\n",
       " 348717.8176194717,\n",
       " 693578.0373698194,\n",
       " 482833.88588594564,\n",
       " 320928.14834389696,\n",
       " 526744.3581575715,\n",
       " 346181.84349514014,\n",
       " 562158.105336665,\n",
       " 788435.3963101793,\n",
       " 1053979.6970564928,\n",
       " 332905.79259044444,\n",
       " 535382.5503170366,\n",
       " 1213737.1291171922,\n",
       " 461822.6473980689,\n",
       " 804404.5434249727,\n",
       " 623561.3053726087,\n",
       " 459917.64570954314,\n",
       " 813802.4416704531,\n",
       " 700148.4857548701,\n",
       " 251625.40103053255,\n",
       " 373630.6483950763,\n",
       " 285305.9457181065,\n",
       " 432582.21986891574,\n",
       " 530634.0713134443,\n",
       " 341270.0972542218,\n",
       " 487235.0756908711,\n",
       " 543809.0149467487,\n",
       " 411543.5434875183,\n",
       " 495731.9651487426,\n",
       " 357130.51120698266,\n",
       " 830418.7460841773,\n",
       " 343391.2704669686,\n",
       " 372353.89962725586,\n",
       " 375095.99242719327,\n",
       " 1189625.1912810665,\n",
       " 463507.27031490504,\n",
       " 286862.729637578,\n",
       " 283731.1219339523,\n",
       " 465306.93791319884,\n",
       " 821639.6194362422,\n",
       " 474307.5149272598,\n",
       " 665797.599276901,\n",
       " 529581.1398737471,\n",
       " 447595.56693171116,\n",
       " 1018481.6168284264,\n",
       " 282868.28193097835,\n",
       " 908609.5516584897,\n",
       " 633875.4238408379,\n",
       " 513737.08642473666,\n",
       " 1033472.9414667111,\n",
       " 462501.75619114295,\n",
       " 278941.1974911297,\n",
       " 848915.7544794254,\n",
       " 331416.48238650605,\n",
       " 627180.3842067027,\n",
       " 1510037.8133118986,\n",
       " 344883.2789764154,\n",
       " 499751.45425600605,\n",
       " 450533.02977385157,\n",
       " 367968.9981281686,\n",
       " 253266.4012807624,\n",
       " 851463.4607661444,\n",
       " 443970.0988524205,\n",
       " 287188.54811853386,\n",
       " 473016.543323549,\n",
       " 355656.3521007969,\n",
       " 283182.8615323244,\n",
       " 497179.1254314623,\n",
       " 353279.4499344843,\n",
       " 412508.6914429595,\n",
       " 392450.372451357,\n",
       " 473881.4004811643,\n",
       " 483060.5053972399,\n",
       " 543315.4702950794,\n",
       " 783472.3654568521,\n",
       " 333802.6396758489,\n",
       " 322685.1873765027,\n",
       " 381337.42961180554,\n",
       " 383555.0593715973,\n",
       " 323344.3048133607,\n",
       " 355140.15250104194,\n",
       " 762009.0970064572,\n",
       " 500652.8390073508,\n",
       " 853501.1259615902,\n",
       " 845687.2206507667,\n",
       " 476086.46405208146,\n",
       " 450241.7689338966,\n",
       " 481157.3450437748,\n",
       " 317677.10932493093,\n",
       " 400317.25933957833,\n",
       " 437655.0831281405,\n",
       " 318147.69339883083,\n",
       " 474541.0617788287,\n",
       " 665797.599276901,\n",
       " 416668.58789721126,\n",
       " 399594.151136652,\n",
       " 463001.9148284745,\n",
       " 415834.6696399181,\n",
       " 1000139.4964166416,\n",
       " 774015.6881040464,\n",
       " 736146.6846366965,\n",
       " 364751.23600868514,\n",
       " 458856.49284188746,\n",
       " 402537.7989192818,\n",
       " 571764.6217097305,\n",
       " 272121.0099201171,\n",
       " 390717.7967631688,\n",
       " 358837.93980994076,\n",
       " 317483.6341506326,\n",
       " 357564.25190697436,\n",
       " 284757.3555186832,\n",
       " 417319.8184210649,\n",
       " 588584.1406983794,\n",
       " 312807.39177986694,\n",
       " 442014.59129373275,\n",
       " 513774.01282520517,\n",
       " 491112.7550838975,\n",
       " 357046.10798333527,\n",
       " 335068.89763083064,\n",
       " 283670.6494240466,\n",
       " 539110.4309749424,\n",
       " 474516.7427262294,\n",
       " 1697962.1121478267,\n",
       " 459917.64570954314,\n",
       " 2131569.0696183667,\n",
       " 678856.895609241,\n",
       " 1003065.6590521332,\n",
       " 625427.0907026313,\n",
       " 412898.2182841751,\n",
       " 459789.49921780825,\n",
       " 929351.4134582167,\n",
       " 691478.2971805438,\n",
       " 476025.53112884675,\n",
       " 465348.57868724,\n",
       " 753052.3833350731,\n",
       " 501500.5019550502,\n",
       " 383020.37791088957,\n",
       " 402820.6295647348,\n",
       " 364164.6953347893,\n",
       " 471093.5499700929,\n",
       " 340951.5366042118,\n",
       " 719004.9934636926,\n",
       " 507272.350456596,\n",
       " 803854.6574323,\n",
       " 693226.3195267676,\n",
       " 509315.79738728714,\n",
       " 449843.08750056307,\n",
       " 482550.3402969322,\n",
       " 492675.12417261564,\n",
       " 505896.4278952001,\n",
       " 565097.6054083386,\n",
       " 309111.7517516444,\n",
       " 1045835.5694397831,\n",
       " 312744.8413496303,\n",
       " 577742.216190608,\n",
       " 275560.0883082009,\n",
       " 1649392.7952880666,\n",
       " 853520.1800556345,\n",
       " 619099.7332211866,\n",
       " 496488.36725902476,\n",
       " 629180.7030071487,\n",
       " 561350.4536556184,\n",
       " 347365.0054089789,\n",
       " 533600.4135102091,\n",
       " 466256.68475603394,\n",
       " 992131.5739762995,\n",
       " 378748.0088751013,\n",
       " 503592.6592054664,\n",
       " 974101.6879634876,\n",
       " 532367.1374145291,\n",
       " 454679.35105927556,\n",
       " 450451.7325137638,\n",
       " 1047642.5984363485,\n",
       " 324437.3004248737,\n",
       " 631818.888579898,\n",
       " 790545.7492564982,\n",
       " 324437.3004248737,\n",
       " 624998.9123471987,\n",
       " 592647.3604629671,\n",
       " 395389.8858554218,\n",
       " 473873.0554215042,\n",
       " 624335.1329779227,\n",
       " 434727.96684248454,\n",
       " 350477.94563501736,\n",
       " 523748.86775191687,\n",
       " 275560.0883082009,\n",
       " 349425.55928817554,\n",
       " 471184.5687830856,\n",
       " 532943.6509461683,\n",
       " 443437.00391372666,\n",
       " 815435.056953638,\n",
       " 405427.0520675791,\n",
       " 298688.99246290093,\n",
       " 319109.38354166964,\n",
       " 471142.8144789544,\n",
       " 729321.3790672184,\n",
       " 746547.7238967761,\n",
       " 372359.63222407707,\n",
       " 298660.484487045,\n",
       " 386023.78347662906,\n",
       " 274829.0023621704,\n",
       " 851656.9161144681,\n",
       " 401229.25106686394,\n",
       " 286450.7349522355,\n",
       " 339105.1833289518,\n",
       " 495294.38922242576,\n",
       " 504619.6214888911,\n",
       " 471142.8144789544,\n",
       " 1131430.3058508807,\n",
       " 1038123.2537429656,\n",
       " 1143875.37782767,\n",
       " 429470.15761753434,\n",
       " 576654.5379884554,\n",
       " 368110.30085415364,\n",
       " 509934.4078673759,\n",
       " 358797.31616593944,\n",
       " 390220.16369933385,\n",
       " 348623.4491128123,\n",
       " 811876.092162475,\n",
       " 468610.7481596296,\n",
       " 361444.9045080362,\n",
       " 392631.3242580214,\n",
       " 297893.14139029826,\n",
       " 529441.5048001454,\n",
       " 483282.18892931036,\n",
       " 417819.63626662234,\n",
       " 529769.644177722,\n",
       " 276857.20383873244,\n",
       " 633516.7819931735,\n",
       " 528982.5196718659,\n",
       " 455078.41963425756,\n",
       " 473690.7189905195,\n",
       " 298360.7205651201,\n",
       " 361429.2160779924,\n",
       " 1066121.3347188304,\n",
       " 765497.4890258292,\n",
       " 450257.9520615883,\n",
       " 324916.25445614423,\n",
       " 347885.2966427794,\n",
       " 1005570.0760496573,\n",
       " 548830.4126884413,\n",
       " 431505.0318624604,\n",
       " 274230.1838992408,\n",
       " 377626.5993900529,\n",
       " 294547.1936158772,\n",
       " 839701.5886164556,\n",
       " 253005.69489243967,\n",
       " 321834.1032395317,\n",
       " 294410.82522664394,\n",
       " 471382.1835870997,\n",
       " 479264.51034836925,\n",
       " 593167.6985512377,\n",
       " 569713.706880427,\n",
       " 383428.0294247013,\n",
       " 473958.3246136539,\n",
       " 561253.8840612343,\n",
       " 484900.96427184466,\n",
       " 472873.58147769514,\n",
       " 616444.0013211285,\n",
       " 510626.68669652083,\n",
       " 777576.0396957961,\n",
       " 383512.24441037624,\n",
       " 314011.9426600562,\n",
       " 468247.9845210379,\n",
       " 584031.5938141061,\n",
       " 473728.7769291607,\n",
       " 489707.041426244,\n",
       " 914784.0739162231,\n",
       " 530236.940095455,\n",
       " 736460.3340970546,\n",
       " 621651.2527321596,\n",
       " 818618.4652316511,\n",
       " 275263.7751571271,\n",
       " 468400.12067228125,\n",
       " 465344.59374478424,\n",
       " 833573.668882641,\n",
       " 896016.7988321639,\n",
       " 273438.14035563456,\n",
       " 261396.63567895,\n",
       " 496513.8105696154,\n",
       " 300091.94368863804,\n",
       " 713323.163134578,\n",
       " 392487.88637109264,\n",
       " 476410.25054081035,\n",
       " 512739.76141618995,\n",
       " 383623.09165736404,\n",
       " 294410.82522664394,\n",
       " 296387.18451081426,\n",
       " 330951.60402050416,\n",
       " 630118.2737353,\n",
       " 284776.7653303259,\n",
       " 368282.3088707318,\n",
       " 476681.803561089,\n",
       " 387346.6770311935,\n",
       " 516329.7439185677,\n",
       " 370842.5206707887,\n",
       " 390220.16369933385,\n",
       " 468610.7481596296,\n",
       " 357503.73011599144,\n",
       " 1148129.869428309,\n",
       " 631379.2368318574,\n",
       " 328663.70021692046,\n",
       " 454941.0397279402,\n",
       " 753402.936528588,\n",
       " 636397.6694378769,\n",
       " 273368.8307305867,\n",
       " 470431.6531728034,\n",
       " 394907.5424673695,\n",
       " 298451.8756935317,\n",
       " 875153.6338058155,\n",
       " 473775.0473593306,\n",
       " 549481.2019949371,\n",
       " 480613.949649668,\n",
       " 680932.1262225029,\n",
       " 572894.7369113477,\n",
       " 853678.4036211267,\n",
       " 657523.866727101,\n",
       " 786977.6825139669,\n",
       " 376015.1494150297,\n",
       " 1032474.1992530364,\n",
       " 596122.9700585486,\n",
       " 769429.0442562301,\n",
       " 455371.0741565515,\n",
       " 468418.92205033347,\n",
       " 378046.19467399124,\n",
       " 468610.7481596296,\n",
       " 373630.6483950763,\n",
       " 781669.0831477452,\n",
       " 525227.5987304717,\n",
       " 486851.7432237819,\n",
       " 280592.7866086684,\n",
       " 275560.0883082009,\n",
       " 300441.7038662623,\n",
       " 792846.7727423037,\n",
       " 650745.126674524,\n",
       " 462646.99475595204,\n",
       " 359141.9360206643,\n",
       " 480878.312556269,\n",
       " 365597.41455893335,\n",
       " 652033.5775792116,\n",
       " 274637.52949500276,\n",
       " 645000.628812159,\n",
       " 528253.0421771219,\n",
       " 350525.3243214196,\n",
       " 279457.19442193804,\n",
       " 484653.7631239838,\n",
       " 505959.0986752103,\n",
       " 378809.14380885335,\n",
       " 688297.1783446192,\n",
       " 335657.68144710007,\n",
       " 392487.88637109264,\n",
       " 539912.9827621015,\n",
       " 304455.9423491309,\n",
       " 567034.0377585063,\n",
       " 687366.3431511407,\n",
       " 694967.3096370678,\n",
       " 487954.2884874153,\n",
       " 421508.14501893165,\n",
       " 714262.223255352,\n",
       " 514920.3567198386,\n",
       " 650745.126674524,\n",
       " 1612735.0688415957,\n",
       " 549313.0250327984,\n",
       " 346717.08756051643,\n",
       " 687161.9513417986,\n",
       " 857110.7180402235,\n",
       " 342250.26530536765,\n",
       " 398436.79142674216,\n",
       " 376490.5011659452,\n",
       " 293667.7171863394,\n",
       " 508191.1111136905,\n",
       " 508974.29146744433,\n",
       " 406133.984991172,\n",
       " 492950.0658881475,\n",
       " 330143.0261211068,\n",
       " 365857.86696960306,\n",
       " 254966.65638820012,\n",
       " 352799.87946418085,\n",
       " 344506.0275483585,\n",
       " 328889.9871966049,\n",
       " 342656.24540266703,\n",
       " 331630.72122013016,\n",
       " 814556.1052606603,\n",
       " 419976.1334135655,\n",
       " 445649.8572193426,\n",
       " 713594.5794210284,\n",
       " 353550.4651913938,\n",
       " 253005.69489243967,\n",
       " 366480.26410294784,\n",
       " 299269.34612758615,\n",
       " 585427.5552564855,\n",
       " 299948.0921950959,\n",
       " 523848.1237714542,\n",
       " 806893.7156757121,\n",
       " 579000.3685754962,\n",
       " 548465.1995790815,\n",
       " 499428.4026959919,\n",
       " 266970.23586795095,\n",
       " 973615.8560863876,\n",
       " 637409.6677421046,\n",
       " 850194.5556090085,\n",
       " 330490.0233524727,\n",
       " 465629.59374652023,\n",
       " 790592.080577287,\n",
       " 515775.8631264977,\n",
       " 257218.98780145805,\n",
       " 1179313.5926189658,\n",
       " 314190.7496774198,\n",
       " 631089.5269907976,\n",
       " 473016.543323549,\n",
       " 757527.7479204645,\n",
       " 545832.5326680512,\n",
       " 452437.6137760562,\n",
       " 458856.49284188746,\n",
       " 449237.4722659571,\n",
       " 548914.2565704507,\n",
       " 1282882.638372564,\n",
       " 479125.49077402987,\n",
       " 830898.5274742338,\n",
       " 485787.6021023132,\n",
       " 377357.4331053005,\n",
       " 276898.20239865716,\n",
       " 433565.00269565854,\n",
       " 553952.6853908405,\n",
       " 493700.8481924605,\n",
       " 456684.5405759569,\n",
       " 294547.1936158772,\n",
       " 364118.402235722,\n",
       " 351490.5523470457,\n",
       " 510773.6387187184,\n",
       " 555802.385929507,\n",
       " 283856.23107478797,\n",
       " 377511.37773945567,\n",
       " 499269.7316332968,\n",
       " 624998.9123471987,\n",
       " 958970.5281256998,\n",
       " 658125.5170250726,\n",
       " 358049.03133688716,\n",
       " 775274.3322188157,\n",
       " 711225.1038938202,\n",
       " 859401.6750335796,\n",
       " 354866.5662538227,\n",
       " 2405098.93980939,\n",
       " 371810.01281894674,\n",
       " 383706.18886214466,\n",
       " 500461.5186436124,\n",
       " 332392.9629251878,\n",
       " 357199.68698830286,\n",
       " 550629.7658122295,\n",
       " 673169.1601696031,\n",
       " 673794.8120112469,\n",
       " 274263.67658507376,\n",
       " 349022.77769245364,\n",
       " 538851.5013555193,\n",
       " 2306211.0415621116,\n",
       " 548571.233579218,\n",
       " 468178.36299020535,\n",
       " 939864.6128243536,\n",
       " 618944.5830645656,\n",
       " 542730.4491681755,\n",
       " 774151.1157511384,\n",
       " 708962.2656118466,\n",
       " 452437.6137760562,\n",
       " 296124.55521828483,\n",
       " 437529.4927567622,\n",
       " 648460.2804054193,\n",
       " 553396.9361747804,\n",
       " 282897.5679145509,\n",
       " 298357.019480526,\n",
       " 558359.8605986447,\n",
       " 410402.99030512664,\n",
       " 668426.9607271482,\n",
       " 494648.41818225366,\n",
       " 943690.7790533019,\n",
       " 375707.35632279684,\n",
       " 291864.3962951972,\n",
       " 393556.26594800974,\n",
       " 289286.59384635714,\n",
       " 361100.51706436044,\n",
       " 342250.26530536765,\n",
       " 385871.9689132794,\n",
       " 899119.4944727847,\n",
       " 572155.7938546789,\n",
       " 362781.37121484394,\n",
       " 509026.43198083993,\n",
       " 657972.5392072378,\n",
       " 257315.19557509577,\n",
       " 533009.2925155297,\n",
       " 370135.6431771229,\n",
       " 638712.1109583748,\n",
       " 642126.1498712123,\n",
       " 466538.899473809,\n",
       " 288088.6262877677,\n",
       " 436838.3173929003,\n",
       " 348481.02943405474,\n",
       " 869080.5781565517,\n",
       " 357375.62112705474,\n",
       " 575701.6219423076,\n",
       " 408371.83622667595,\n",
       " 387900.2121693505,\n",
       " 351129.2364115592,\n",
       " 343267.4379935578,\n",
       " ...]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bagging_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
